# Automated Trading System

**High-performance automated trading system with machine learning, real-time market data, and complete automation.**

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![Docker](https://img.shields.io/badge/Docker-20.10+-blue.svg)](https://www.docker.com/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15+-blue.svg)](https://www.postgresql.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## ğŸš€ Quick Start

```bash
# 1. Start all services (Docker)
./run.sh dev

# 2. Access web interface
http://localhost:5001

# 3. Access admin dashboard (admin users only)
http://localhost:5001/admin

# 4. Check system status
./tools/check_all_schedulers.sh
```

**First run:** Initial pipeline takes ~30 minutes to populate data for 2,259 stocks.

**Subsequent runs:** Fully automated - no manual intervention needed!

---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [System Overview](#-system-overview)
- [Installation](#-installation)
- [Architecture](#-architecture)
- [Database Schema](#ï¸-database-schema)
- [API Endpoints](#-api-endpoints)
- [Automation](#-automation)
- [Machine Learning](#-machine-learning)
- [Configuration](#ï¸-configuration)
- [Usage Examples](#-usage-examples)
- [Monitoring](#-monitoring)
- [Troubleshooting](#-troubleshooting)
- [Performance](#-performance)
- [Documentation](#-documentation)
- [Contributing](#-contributing)

---

## âœ¨ Features

### ğŸ¤– Complete Automation
- **100% Automated Data Collection** - Runs daily at 9 PM after market close
- **Automated ML Training** - Trains models daily at 2 AM with fresh data
- **Automated Stock Selection** - Generates daily top 50 stock picks
- **Automated CSV Exports** - Daily backups for analysis and reporting
- **Self-Healing** - Auto-restart on failures, retry failed steps
- **Zero Manual Intervention** - Set it and forget it!

### ğŸ§  Machine Learning
- **Random Forest Models** - Price prediction and risk assessment
- **2-Week Price Targets** - ML-predicted target prices with confidence scores
- **Risk Assessment** - Maximum drawdown prediction (0-1 score)
- **25-30 Features** - Technical indicators + fundamental metrics
- **Fast Training** - Only 1-2 minutes for 600K+ samples
- **Daily Updates** - Models retrained with latest market data

### ğŸ“Š Comprehensive Data Pipeline
- **6-Step Saga Pattern** - Symbols â†’ Stocks â†’ History â†’ Indicators â†’ Metrics â†’ Validation
- **2,259 Stocks** - All NSE-listed stocks with complete fundamental data
- **1-Year Historical Data** - 820K+ OHLCV records
- **Technical Indicators** - RSI, MACD, SMA, EMA, ATR, Bollinger Bands
- **Real-Time Updates** - Market data refresh every 30 minutes during trading hours
- **Robust Error Handling** - Retry logic, compensation, saga rollback

### ğŸŒ API & Interface
- **REST API** - Fast, cached endpoints with ML predictions
- **Admin Dashboard** - Manual task triggers with real-time monitoring
- **Web Interface** - Portfolio management and trading controls
- **Multi-Broker Support** - Fyers, Zerodha (Kite), Simulator
- **Real-Time Status** - Live task monitoring and logs
- **Retry Mechanism** - Smart retry for failed tasks and individual steps

### ğŸ“ˆ Analytics & Insights
- **ML Prediction Scores** - 0-1 score for opportunity ranking
- **Confidence Levels** - Model confidence in predictions
- **Risk Scores** - Lower = safer investment
- **Strategy-Based Filtering** - Growth, Value, Balanced, Momentum strategies
- **Sector Analysis** - Performance across different sectors
- **Historical Backtesting** - Test strategies on historical data

---

## ğŸ“Š System Overview

### Daily Automation Schedule

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AUTOMATED DAILY SCHEDULE                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸŒ… Morning (Monday Only):
  06:00 AM â†’ Symbol Master Update
             â€¢ Refresh NSE symbols from Fyers API
             â€¢ ~2,300 stocks
             â€¢ Duration: 1-2 minutes

ğŸŒ† Evening (Daily After Market Close):
  21:00 PM â†’ Data Pipeline (6-step saga)
             â€¢ Update all stock prices
             â€¢ Fetch 1-year historical OHLCV
             â€¢ Calculate technical indicators
             â€¢ Compute volatility metrics
             â€¢ Validate data quality
             â€¢ Duration: 20-30 minutes
             â€¢ Records: 2,259 stocks updated

  21:30 PM â†’ Fill Missing Data
             â€¢ Populate adj_close, liquidity
             â€¢ Calculate ATR, volatility, volume averages
             â€¢ Duration: 2-5 minutes

  21:45 PM â†’ Business Logic Calculations
             â€¢ EPS, Book Value, PEG Ratio, ROA
             â€¢ Operating/Net/Profit Margins
             â€¢ Current/Quick Ratios
             â€¢ Debt to Equity, Growth metrics
             â€¢ Duration: 2-5 minutes

  22:00 PM â†’ CSV Export
             â€¢ stocks_YYYY-MM-DD.csv (all stocks)
             â€¢ historical_30d_YYYY-MM-DD.csv (30 days OHLCV)
             â€¢ technical_indicators_YYYY-MM-DD.csv (indicators)
             â€¢ suggested_stocks_YYYY-MM-DD.csv (top picks)
             â€¢ Duration: 1-2 minutes

  22:30 PM â†’ Data Quality Validation
             â€¢ Check record counts
             â€¢ Verify data freshness
             â€¢ Detect anomalies
             â€¢ Duration: < 1 minute

ğŸŒ™ Early Morning (Next Day):
  02:00 AM â†’ ML Model Training
             â€¢ Train Random Forest price model
             â€¢ Train Random Forest risk model
             â€¢ Use 365 days historical data
             â€¢ 25-30 features (technical + fundamental)
             â€¢ Duration: 1-2 minutes

  02:15 AM â†’ Daily Stock Selection
             â€¢ Run suggested stocks saga
             â€¢ Apply ML predictions to all stocks
             â€¢ Select top 50 stocks
             â€¢ Save to daily_suggested_stocks table
             â€¢ Duration: 2-3 minutes

  03:00 AM â†’ Cleanup Old Data (Sunday Only)
             â€¢ Remove snapshots > 90 days
             â€¢ Remove CSV exports > 30 days
             â€¢ Duration: < 1 minute

Total Daily Automation Time: ~40-50 minutes
Total Manual Intervention: ZERO! ğŸ‰
```

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SYSTEM ARCHITECTURE                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

External APIs:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Fyers API   â”‚â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ Zerodha API  â”‚â”€â”€â”¤
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                  â”‚
                  â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Flask App     â”‚ â† HTTP Requests
         â”‚  (Port 5001)   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Redis Cache   â”‚
         â”‚  (Dragonfly)   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    PostgreSQL DB        â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚ stocks (2,259)   â”‚   â”‚
    â”‚  â”‚ historical_data  â”‚   â”‚
    â”‚  â”‚ tech_indicators  â”‚   â”‚
    â”‚  â”‚ daily_suggested  â”‚   â”‚
    â”‚  â”‚ pipeline_track   â”‚   â”‚
    â”‚  â”‚ users, orders    â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†‘
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Schedulerâ”‚      â”‚ ML Scheduler â”‚
â”‚  (9 PM daily) â”‚      â”‚ (2 AM daily) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Storage:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  /exports  â”‚        â”‚   /logs    â”‚
â”‚ (CSV files)â”‚        â”‚ (App logs) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Installation

### Prerequisites

- **Operating System:** Linux, macOS, or Windows (WSL2)
- **Docker:** Version 20.10 or higher
- **Docker Compose:** Version 2.0 or higher
- **Python:** 3.10 or higher (for local scripts)
- **Git:** For cloning the repository
- **Fyers API Credentials:** Client ID and Access Token

### Step 1: Clone Repository

```bash
git clone https://github.com/yourusername/StockExperiment.git
cd StockExperiment
```

### Step 2: Configure Fyers API

Create `.env` file in root:

```bash
# Fyers API Credentials
FYERS_CLIENT_ID=your_client_id_here
FYERS_ACCESS_TOKEN=your_access_token_here

# Database Configuration (default - don't change unless needed)
DATABASE_URL=postgresql://trader:trader_password@database:5432/trading_system
POSTGRES_USER=trader
POSTGRES_PASSWORD=trader_password
POSTGRES_DB=trading_system

# Redis Configuration
REDIS_HOST=dragonfly
REDIS_PORT=6379

# Application Settings
FLASK_ENV=development
FLASK_DEBUG=1
LOG_LEVEL=INFO
```

### Step 3: Start Docker Services

```bash
# Development mode (with auto-reload)
./run.sh dev

# Production mode
./run.sh prod
```

### Step 4: Wait for Initial Setup

```bash
# Monitor initial pipeline (takes ~30 minutes)
docker compose logs -f trading_system

# Check when ready
./tools/check_all_schedulers.sh
```

### Step 5: Access Application

```bash
# Web Interface
http://localhost:5001

# Admin Dashboard
http://localhost:5001/admin

# API Documentation
http://localhost:5001/api/suggested-stocks/

# Database (PostgreSQL)
docker exec -it trading_system_db_dev psql -U trader -d trading_system
```

---

## ğŸ—ï¸ Architecture

### Technology Stack

**Backend:**
- **Python 3.10+** - Core language
- **Flask 3.0** - Web framework
- **SQLAlchemy 2.0** - ORM for database
- **PostgreSQL 15** - Primary database
- **Dragonfly (Redis)** - Caching layer
- **Scikit-learn** - Machine learning models
- **Pandas/NumPy** - Data processing
- **Schedule** - Task scheduling

**Frontend:**
- **Bootstrap 5** - UI framework
- **Chart.js** - Data visualization
- **JavaScript ES6** - Client-side logic
- **Jinja2** - Template engine

**DevOps:**
- **Docker & Docker Compose** - Containerization
- **Git** - Version control
- **GitHub Actions** - CI/CD (optional)

### Directory Structure

```
/StockExperiment
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ .env                          # Environment variables (create this)
â”œâ”€â”€ .gitignore                    # Git ignore rules
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ Dockerfile                    # Docker image definition
â”œâ”€â”€ docker-compose.yml            # Docker services config
â”‚
â”œâ”€â”€ app.py                        # Flask application entry
â”œâ”€â”€ run.py                        # Application launcher
â”œâ”€â”€ run.sh                        # Docker startup script
â”œâ”€â”€ config.py                     # Application configuration
â”‚
â”œâ”€â”€ run_pipeline.py               # Data pipeline orchestrator
â”œâ”€â”€ data_scheduler.py             # Data automation (9 PM)
â”œâ”€â”€ scheduler.py                  # ML automation (2 AM)
â”‚
â”œâ”€â”€ /src                          # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ /models                   # Database models (SQLAlchemy ORM)
â”‚   â”‚   â”œâ”€â”€ database.py           # DB connection, session management
â”‚   â”‚   â””â”€â”€ models.py             # Table definitions (11 tables)
â”‚   â”‚
â”‚   â”œâ”€â”€ /services                 # Business logic
â”‚   â”‚   â”œâ”€â”€ /data                 # Data pipeline services
â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline_saga.py  # 6-step data saga
â”‚   â”‚   â”‚   â”œâ”€â”€ suggested_stocks_saga.py  # Stock selection saga
â”‚   â”‚   â”‚   â””â”€â”€ daily_snapshot_service.py # Daily snapshot logic
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ /broker               # Broker integrations
â”‚   â”‚   â”‚   â”œâ”€â”€ fyers_service.py  # Fyers API wrapper
â”‚   â”‚   â”‚   â””â”€â”€ zerodha_service.py # Zerodha API wrapper
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ /ml                   # Machine learning
â”‚   â”‚       â”œâ”€â”€ stock_predictor.py # Random Forest models
â”‚   â”‚       â””â”€â”€ config_loader.py   # ML configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ /web                      # Flask routes & templates
â”‚   â”‚   â”œâ”€â”€ app.py                # Flask app factory
â”‚   â”‚   â”œâ”€â”€ admin_routes.py       # Admin dashboard routes
â”‚   â”‚   â”œâ”€â”€ /routes               # API routes
â”‚   â”‚   â”‚   â””â”€â”€ suggested_stocks_routes.py
â”‚   â”‚   â””â”€â”€ /templates            # HTML templates
â”‚   â”‚       â”œâ”€â”€ base.html         # Base template
â”‚   â”‚       â”œâ”€â”€ dashboard.html    # Main dashboard
â”‚   â”‚       â””â”€â”€ /admin            # Admin templates
â”‚   â”‚           â””â”€â”€ dashboard.html
â”‚   â”‚
â”‚   â””â”€â”€ /utils                    # Helper utilities
â”‚       â”œâ”€â”€ logger.py             # Logging setup
â”‚       â””â”€â”€ helpers.py            # Common utilities
â”‚
â”œâ”€â”€ /config                       # Configuration files
â”‚   â”œâ”€â”€ stock_filters.yaml        # Stock screening criteria
â”‚   â”œâ”€â”€ database.yaml             # Database settings
â”‚   â””â”€â”€ broker_config.yaml        # Broker settings
â”‚
â”œâ”€â”€ /docker                       # Docker files
â”‚   â”œâ”€â”€ docker-compose.yml        # Services definition
â”‚   â””â”€â”€ /database                 # Database files
â”‚
â”œâ”€â”€ /tools                        # Utility scripts
â”‚   â”œâ”€â”€ README.md                 # Tools documentation
â”‚   â”œâ”€â”€ train_ml_model.py         # Manual ML training
â”‚   â”œâ”€â”€ check_scheduler.sh        # ML scheduler status
â”‚   â””â”€â”€ check_all_schedulers.sh   # Complete system status
â”‚
â”œâ”€â”€ /docs                         # Documentation
â”‚   â”œâ”€â”€ AUTOMATION.md             # Automation guide
â”‚   â”œâ”€â”€ ML_GUIDE.md               # ML guide
â”‚   â”œâ”€â”€ STRUCTURE.md              # Project structure
â”‚   â””â”€â”€ /_archive                 # Old docs
â”‚
â”œâ”€â”€ /logs                         # Application logs
â”‚   â”œâ”€â”€ data_scheduler.log        # Data pipeline logs
â”‚   â”œâ”€â”€ scheduler.log             # ML scheduler logs
â”‚   â””â”€â”€ app.log                   # Flask app logs
â”‚
â””â”€â”€ /exports                      # CSV exports (daily)
    â”œâ”€â”€ stocks_YYYY-MM-DD.csv
    â”œâ”€â”€ historical_30d_YYYY-MM-DD.csv
    â”œâ”€â”€ technical_indicators_YYYY-MM-DD.csv
    â””â”€â”€ suggested_stocks_YYYY-MM-DD.csv
```

### Docker Services

```yaml
services:
  database:
    image: postgres:15
    container_name: trading_system_db_dev
    ports: ["5432:5432"]
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: trader
      POSTGRES_PASSWORD: trader_password
      POSTGRES_DB: trading_system

  dragonfly:
    image: docker.dragonflydb.io/dragonflydb/dragonfly
    container_name: trading_system_redis
    ports: ["6379:6379"]
    volumes:
      - dragonfly_data:/data

  trading_system:
    build: .
    container_name: trading_system_app_dev
    ports: ["5001:5001"]
    depends_on: [database, dragonfly]
    volumes:
      - .:/app
      - ./logs:/app/logs
      - ./exports:/app/exports
    environment:
      DATABASE_URL: postgresql://trader:trader_password@database:5432/trading_system
      REDIS_HOST: dragonfly

  data_scheduler:
    build: .
    container_name: trading_system_data_scheduler
    command: python data_scheduler.py
    depends_on: [database, dragonfly]
    volumes:
      - .:/app
      - ./logs:/app/logs
      - ./exports:/app/exports

  ml_scheduler:
    build: .
    container_name: trading_system_ml_scheduler
    command: python scheduler.py
    depends_on: [database, dragonfly]
    volumes:
      - .:/app
      - ./logs:/app/logs
```

---

## ğŸ—„ï¸ Database Schema

### Table Overview (11 Tables Total)

| Table | Records | Description |
|-------|---------|-------------|
| **stocks** | 2,259 | Current prices, fundamentals, ratios |
| **historical_data** | ~820,000 | 1-year OHLCV data |
| **technical_indicators** | ~820,000 | RSI, MACD, SMA, EMA, ATR |
| **daily_suggested_stocks** | ~4,500 | Daily top 50 picks with ML |
| **pipeline_execution_tracking** | Variable | Pipeline saga logs |
| **users** | Variable | User accounts |
| **strategies** | Variable | Trading strategies |
| **orders** | Variable | Order history |
| **trades** | Variable | Executed trades |
| **positions** | Variable | Current positions |
| **broker_configurations** | Variable | API credentials |

### Core Tables Schema

#### `stocks` Table
```sql
CREATE TABLE stocks (
    id SERIAL PRIMARY KEY,
    symbol VARCHAR(50) UNIQUE NOT NULL,
    stock_name VARCHAR(200),

    -- Price & Market Data
    current_price DOUBLE PRECISION,
    market_cap DOUBLE PRECISION,
    volume BIGINT,
    avg_volume_30d BIGINT,

    -- Fundamental Ratios
    pe_ratio DOUBLE PRECISION,
    pb_ratio DOUBLE PRECISION,
    roe DOUBLE PRECISION,
    eps DOUBLE PRECISION,
    beta DOUBLE PRECISION,
    debt_to_equity DOUBLE PRECISION,

    -- Growth & Profitability
    revenue_growth DOUBLE PRECISION,
    earnings_growth DOUBLE PRECISION,
    operating_margin DOUBLE PRECISION,
    net_margin DOUBLE PRECISION,
    profit_margin DOUBLE PRECISION,

    -- Valuation
    book_value DOUBLE PRECISION,
    peg_ratio DOUBLE PRECISION,
    roa DOUBLE PRECISION,

    -- Liquidity
    current_ratio DOUBLE PRECISION,
    quick_ratio DOUBLE PRECISION,

    -- Volatility
    historical_volatility_1y DOUBLE PRECISION,
    atr_14 DOUBLE PRECISION,

    -- Metadata
    sector VARCHAR(100),
    industry VARCHAR(100),
    market_cap_category VARCHAR(20),
    last_updated TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### `historical_data` Table
```sql
CREATE TABLE historical_data (
    id SERIAL PRIMARY KEY,
    symbol VARCHAR(50) NOT NULL,
    date DATE NOT NULL,
    open DOUBLE PRECISION,
    high DOUBLE PRECISION,
    low DOUBLE PRECISION,
    close DOUBLE PRECISION,
    adj_close DOUBLE PRECISION,
    volume BIGINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    UNIQUE(symbol, date),
    FOREIGN KEY (symbol) REFERENCES stocks(symbol)
);
CREATE INDEX idx_hist_symbol_date ON historical_data(symbol, date DESC);
```

#### `technical_indicators` Table
```sql
CREATE TABLE technical_indicators (
    id SERIAL PRIMARY KEY,
    symbol VARCHAR(50) NOT NULL,
    date DATE NOT NULL,

    -- Momentum Indicators
    rsi_14 DOUBLE PRECISION,

    -- Trend Indicators
    macd DOUBLE PRECISION,
    signal_line DOUBLE PRECISION,
    macd_histogram DOUBLE PRECISION,
    sma_20 DOUBLE PRECISION,
    sma_50 DOUBLE PRECISION,
    sma_200 DOUBLE PRECISION,
    ema_12 DOUBLE PRECISION,
    ema_26 DOUBLE PRECISION,

    -- Volatility Indicators
    atr_14 DOUBLE PRECISION,
    atr_percentage DOUBLE PRECISION,
    bollinger_upper DOUBLE PRECISION,
    bollinger_middle DOUBLE PRECISION,
    bollinger_lower DOUBLE PRECISION,

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    UNIQUE(symbol, date),
    FOREIGN KEY (symbol) REFERENCES stocks(symbol)
);
CREATE INDEX idx_tech_symbol_date ON technical_indicators(symbol, date DESC);
```

#### `daily_suggested_stocks` Table
```sql
CREATE TABLE daily_suggested_stocks (
    id SERIAL PRIMARY KEY,
    date DATE NOT NULL,
    symbol VARCHAR(50) NOT NULL,
    stock_name VARCHAR(200),
    current_price DOUBLE PRECISION,
    market_cap DOUBLE PRECISION,

    -- Strategy & Selection
    strategy VARCHAR(50) NOT NULL,
    selection_score DOUBLE PRECISION,
    rank INTEGER,

    -- ML Predictions
    ml_prediction_score DOUBLE PRECISION,  -- 0-1 (higher = better)
    ml_price_target DOUBLE PRECISION,      -- Predicted price in 2 weeks
    ml_confidence DOUBLE PRECISION,        -- Model confidence 0-1
    ml_risk_score DOUBLE PRECISION,        -- Risk score 0-1 (lower = safer)

    -- Technical Indicators (snapshot)
    rsi_14 DOUBLE PRECISION,
    macd DOUBLE PRECISION,
    sma_50 DOUBLE PRECISION,
    sma_200 DOUBLE PRECISION,

    -- Fundamental Metrics (snapshot)
    pe_ratio DOUBLE PRECISION,
    pb_ratio DOUBLE PRECISION,
    roe DOUBLE PRECISION,
    eps DOUBLE PRECISION,
    beta DOUBLE PRECISION,

    -- Growth & Profitability
    revenue_growth DOUBLE PRECISION,
    earnings_growth DOUBLE PRECISION,
    operating_margin DOUBLE PRECISION,

    -- Trading Signals
    target_price DOUBLE PRECISION,
    stop_loss DOUBLE PRECISION,
    recommendation VARCHAR(20),
    reason TEXT,

    -- Metadata
    sector VARCHAR(100),
    market_cap_category VARCHAR(20),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    UNIQUE(date, symbol, strategy)  -- Upsert key
);
CREATE INDEX idx_daily_suggested_date ON daily_suggested_stocks(date DESC);
CREATE INDEX idx_daily_suggested_ml_score ON daily_suggested_stocks(ml_prediction_score DESC);
```

---

## ğŸŒ API Endpoints

### Main Endpoint: Suggested Stocks

```http
GET /api/suggested-stocks/
```

**Query Parameters:**
- `strategy` - Trading strategy: `growth`, `value`, `balanced`, `momentum` (default: `balanced`)
- `limit` - Number of stocks to return (default: `50`, max: `100`)
- `sector` - Filter by sector (optional)
- `search` - Search by symbol or name (optional)
- `sort_by` - Sort field: `ml_prediction_score`, `market_cap`, `pe_ratio` (default: `ml_prediction_score`)
- `order` - Sort order: `asc`, `desc` (default: `desc`)

**Example Request:**
```bash
curl "http://localhost:5001/api/suggested-stocks/?strategy=balanced&limit=10&sort_by=ml_prediction_score"
```

**Example Response:**
```json
{
  "success": true,
  "count": 10,
  "strategy": "balanced",
  "generated_at": "2025-10-04T14:30:00",
  "stocks": [
    {
      "symbol": "RELIANCE",
      "stock_name": "Reliance Industries Ltd",
      "rank": 1,
      "current_price": 2450.50,
      "market_cap": 16500000.0,
      "sector": "Energy",

      "ml_prediction_score": 0.78,
      "ml_price_target": 2650.20,
      "ml_confidence": 0.85,
      "ml_risk_score": 0.12,

      "technical_indicators": {
        "rsi_14": 58.5,
        "macd": 12.3,
        "sma_50": 2400.0,
        "sma_200": 2300.0
      },

      "fundamentals": {
        "pe_ratio": 24.5,
        "pb_ratio": 2.8,
        "roe": 12.5,
        "eps": 100.5,
        "beta": 1.15,
        "revenue_growth": 8.5,
        "earnings_growth": 10.2,
        "operating_margin": 15.3
      },

      "trading_signals": {
        "target_price": 2650.0,
        "stop_loss": 2300.0,
        "recommendation": "BUY"
      }
    }
  ]
}
```

### Admin Dashboard Endpoints

#### Get System Status
```http
GET /admin/system/status
```

**Response:**
```json
{
  "success": true,
  "status": {
    "stocks": {
      "total": 2259,
      "with_price": 2250,
      "with_market_cap": 2240,
      "last_updated": "2025-10-04T21:30:00"
    },
    "historical_data": {
      "symbols": 2259,
      "records": 820574,
      "latest_date": "2025-10-04"
    },
    "technical_indicators": {
      "symbols": 2259,
      "latest_date": "2025-10-04"
    },
    "daily_snapshots": {
      "total": 4500,
      "unique_dates": 90,
      "latest_date": "2025-10-04"
    }
  }
}
```

#### Trigger Tasks
```http
POST /admin/trigger/pipeline           # Run data pipeline
POST /admin/trigger/fill-data          # Fill missing data
POST /admin/trigger/business-logic     # Calculate metrics
POST /admin/trigger/ml-training        # Train ML models
POST /admin/trigger/csv-export         # Export CSV files
POST /admin/trigger/all                # Run all tasks sequentially
```

**Response:**
```json
{
  "success": true,
  "task_id": "pipeline_20251004_143000",
  "message": "Data pipeline started"
}
```

#### Check Task Status
```http
GET /admin/task/{task_id}/status
```

**Response:**
```json
{
  "success": true,
  "task": {
    "task_id": "pipeline_20251004_143000",
    "status": "running",
    "description": "Data Pipeline (6-step saga)",
    "start_time": "2025-10-04T14:30:00",
    "steps": [
      {
        "name": "SYMBOL_MASTER",
        "status": "completed",
        "start_time": "2025-10-04T14:30:00",
        "end_time": "2025-10-04T14:31:00"
      },
      {
        "name": "STOCKS",
        "status": "running",
        "start_time": "2025-10-04T14:31:00"
      }
    ]
  }
}
```

#### Retry Failed Steps
```http
POST /admin/task/{task_id}/retry-failed
```

**Response:**
```json
{
  "success": true,
  "task_id": "retry_20251004_143500",
  "message": "Retrying 2 failed steps",
  "failed_steps": ["HISTORICAL_DATA", "TECHNICAL_INDICATORS"]
}
```

---

## ğŸ¤– Automation

### Data Scheduler (`data_scheduler.py`)

**Schedule:**
- Symbol Master: Monday 6:00 AM
- Data Pipeline: Daily 9:00 PM
- Fill Data: Daily 9:30 PM
- Business Logic: Daily 9:45 PM
- CSV Export: Daily 10:00 PM
- Validation: Daily 10:30 PM

**Configuration:**
```python
# Edit data_scheduler.py to change schedule
schedule.every().monday.at("06:00").do(update_symbol_master)
schedule.every().day.at("21:00").do(run_data_pipeline)
schedule.every().day.at("21:30").do(fill_missing_data)
schedule.every().day.at("21:45").do(calculate_business_logic)
schedule.every().day.at("22:00").do(export_daily_csv)
schedule.every().day.at("22:30").do(validate_data_quality)
```

### ML Scheduler (`scheduler.py`)

**Schedule:**
- ML Training: Daily 2:00 AM
- Daily Snapshot: Daily 2:15 AM
- Cleanup: Sunday 3:00 AM

**Configuration:**
```python
# Edit scheduler.py to change schedule
schedule.every().day.at("02:00").do(train_ml_models)
schedule.every().day.at("02:15").do(update_daily_snapshot)
schedule.every().sunday.at("03:00").do(cleanup_old_snapshots)
```

### Manual Task Execution

**Via Admin Dashboard:**
```
http://localhost:5001/admin
â†’ Click task buttons (Pipeline, ML Training, etc.)
â†’ Monitor progress in real-time
â†’ Retry failed steps
```

**Via Command Line:**
```bash
# Run data pipeline
python3 run_pipeline.py

# Train ML models
python3 tools/train_ml_model.py

# Fill missing data (if pipeline completed)
python3 fill_data_sql.py
python3 fix_business_logic.py
```

---

## ğŸ§  Machine Learning

### Models

**1. Price Prediction Model**
- **Algorithm:** Random Forest Regressor
- **Target:** 2-week price change percentage
- **Output:** `ml_prediction_score` (0-1), `ml_price_target` (â‚¹), `ml_confidence` (0-1)

**2. Risk Assessment Model**
- **Algorithm:** Random Forest Regressor
- **Target:** Maximum drawdown in next 2 weeks
- **Output:** `ml_risk_score` (0-1, lower = safer)

### Features (25-30 total)

**Price & Market:**
- `current_price`, `market_cap`, `volume`

**Fundamentals:**
- `pe_ratio`, `pb_ratio`, `roe`, `eps`, `beta`, `debt_to_equity`

**Growth & Profitability:**
- `revenue_growth`, `earnings_growth`, `operating_margin`, `net_margin`

**Volatility:**
- `historical_volatility_1y`, `atr_14`, `atr_percentage`

**Technical Indicators:**
- `rsi_14`, `macd`, `signal_line`, `macd_histogram`
- `sma_50`, `sma_200`, `ema_12`, `ema_26`

**Engineered Features:**
- `sma_ratio` = sma_50 / sma_200
- `ema_diff` = ema_12 - ema_26
- `price_vs_sma50`, `price_vs_sma200`

### Training Details

**Data:**
- 365 days lookback
- ~600,000-700,000 training samples
- Train/test split not needed (time series)

**Model Configuration:**
```python
RandomForestRegressor(
    n_estimators=100,      # 100 trees (fast training)
    max_depth=10,          # Shallow trees
    min_samples_split=20,  # Prevents overfitting
    min_samples_leaf=10,   # Larger leaves
    n_jobs=-1              # Parallel processing
)
```

**Performance:**
- Price RÂ²: 0.15-0.35 (good for stock prediction!)
- Risk RÂ²: 0.20-0.40
- Training time: 1-2 minutes

### Interpretation

**High Opportunity:**
- `ml_prediction_score > 0.7` + `ml_confidence > 0.7` = Strong buy signal

**Low Risk:**
- `ml_risk_score < 0.3` = Limited downside expected

**Combined Signal:**
- Score >0.7 + Risk <0.3 + Confidence >0.7 = **Strong buy with low risk**
- Score <0.4 + Risk >0.5 = **Avoid**

---

## âš™ï¸ Configuration

### Environment Variables (`.env`)

```bash
# Fyers API (Required)
FYERS_CLIENT_ID=your_fyers_client_id
FYERS_ACCESS_TOKEN=your_fyers_access_token

# Database (Default - don't change unless needed)
DATABASE_URL=postgresql://trader:trader_password@database:5432/trading_system
POSTGRES_USER=trader
POSTGRES_PASSWORD=trader_password
POSTGRES_DB=trading_system

# Redis
REDIS_HOST=dragonfly
REDIS_PORT=6379

# Flask
FLASK_ENV=development
FLASK_DEBUG=1
SECRET_KEY=your_secret_key_here

# Logging
LOG_LEVEL=INFO
LOG_FILE=/app/logs/app.log

# Screening Thresholds (Optional)
SCREENING_MIN_PRICE_THRESHOLD=50.0
SCREENING_MIN_MARKET_CAP_CRORES=500
SCREENING_MIN_VOLUME=100000
```

### Stock Filters (`config/stock_filters.yaml`)

```yaml
# Minimum thresholds
min_price: 50.0
max_price: 5000.0
min_market_cap_crores: 500
min_volume: 100000

# Technical indicators
min_rsi: 30
max_rsi: 70
min_macd: -50
max_macd: 50

# Fundamental ratios
max_pe_ratio: 50
min_roe: 10
max_debt_to_equity: 2.0

# ML scores
min_ml_prediction_score: 0.5
max_ml_risk_score: 0.5
```

---

## ğŸ’¼ Usage Examples

### Example 1: Get Top 10 Growth Stocks

```bash
curl "http://localhost:5001/api/suggested-stocks/?strategy=growth&limit=10"
```

### Example 2: Get Balanced Stocks in IT Sector

```bash
curl "http://localhost:5001/api/suggested-stocks/?strategy=balanced&sector=IT&limit=20"
```

### Example 3: Search for Specific Stock

```bash
curl "http://localhost:5001/api/suggested-stocks/?search=RELIANCE"
```

### Example 4: Manually Trigger Data Pipeline

```python
import requests

response = requests.post('http://localhost:5001/admin/trigger/pipeline')
data = response.json()
print(f"Task ID: {data['task_id']}")

# Check status
status_response = requests.get(f"http://localhost:5001/admin/task/{data['task_id']}/status")
print(status_response.json())
```

### Example 5: Query Database Directly

```bash
docker exec -it trading_system_db_dev psql -U trader -d trading_system

# SQL queries
SELECT COUNT(*) FROM stocks;
SELECT COUNT(*) FROM historical_data;
SELECT * FROM daily_suggested_stocks WHERE date = CURRENT_DATE ORDER BY rank LIMIT 10;
```

---

## ğŸ“Š Monitoring

### System Status

```bash
# Complete system status
./tools/check_all_schedulers.sh

# ML scheduler only
./tools/check_scheduler.sh

# Docker containers
docker compose ps

# View logs (real-time)
docker compose logs -f
docker compose logs -f data_scheduler
docker compose logs -f ml_scheduler
docker compose logs -f trading_system
```

### Database Queries

```sql
-- Latest updates
SELECT
    'Stocks' as table_name,
    COUNT(*) as records,
    MAX(last_updated) as last_update
FROM stocks;

-- Today's top picks
SELECT symbol, stock_name, ml_prediction_score, ml_price_target, rank
FROM daily_suggested_stocks
WHERE date = CURRENT_DATE
ORDER BY rank
LIMIT 10;

-- Data coverage
SELECT
    COUNT(*) as total_stocks,
    COUNT(current_price) as with_price,
    COUNT(DISTINCT h.symbol) as with_history,
    COUNT(DISTINCT ti.symbol) as with_indicators
FROM stocks s
LEFT JOIN historical_data h ON s.symbol = h.symbol
LEFT JOIN technical_indicators ti ON s.symbol = ti.symbol;
```

### Log Files

```bash
# Data scheduler logs
tail -f logs/data_scheduler.log

# ML scheduler logs
tail -f logs/scheduler.log

# Flask app logs
tail -f logs/app.log
```

---

## ğŸ”§ Troubleshooting

### Schedulers Not Running

```bash
# Check container status
docker compose ps

# Restart schedulers
docker compose restart data_scheduler ml_scheduler

# View errors
docker compose logs data_scheduler | grep ERROR
docker compose logs ml_scheduler | grep ERROR
```

### Missing Data

```bash
# Re-run pipeline
python3 run_pipeline.py

# Or via Docker
docker compose exec data_scheduler python3 run_pipeline.py

# Fill missing data
python3 fill_data_sql.py
python3 fix_business_logic.py
```

### Database Connection Issues

```bash
# Check database is running
docker compose ps database

# Test connection
docker exec -it trading_system_db_dev psql -U trader -d trading_system -c "SELECT 1;"

# Restart database
docker compose restart database
```

### ML Training Errors

```bash
# Check data availability
docker exec -it trading_system_db_dev psql -U trader -d trading_system -c "SELECT COUNT(*) FROM historical_data;"

# Manual training
python3 tools/train_ml_model.py

# Check logs
docker compose logs ml_scheduler | tail -50
```

### CSV Exports Not Generated

```bash
# Create directory
mkdir -p exports
chmod 755 exports

# Manual export
docker compose exec data_scheduler python3 -c "from data_scheduler import export_daily_csv; export_daily_csv()"
```

---

## ğŸ“ˆ Performance

### Metrics

- **Data Pipeline:** 20-30 minutes (2,259 stocks)
- **ML Training:** 1-2 minutes (600K+ samples, 100 trees)
- **API Response:** < 100ms (with Redis cache)
- **Stock Sync:** ~113 stocks/second
- **Database Size:** ~1.6M records, ~500MB
- **Memory Usage:** ~500MB (Flask), ~1GB (PostgreSQL)
- **CPU Usage:** Multi-core (parallel processing)

### Optimization Tips

1. **Increase Cache TTL** - Edit Redis cache timeout
2. **Add Indexes** - For frequently queried fields
3. **Batch Processing** - Process stocks in batches
4. **Connection Pooling** - Adjust SQLAlchemy pool size
5. **Reduce Lookback** - Use 180 days instead of 365 for faster training

---

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| **[README.md](README.md)** | This file - comprehensive overview |
| **[AUTOMATION.md](docs/AUTOMATION.md)** | Complete automation guide (schedulers, tasks, monitoring) |
| **[ML_GUIDE.md](docs/ML_GUIDE.md)** | Machine learning models, training, predictions, customization |
| **[STRUCTURE.md](docs/STRUCTURE.md)** | Project structure, data flow, configuration |
| **[tools/README.md](tools/README.md)** | Utility scripts documentation |

---

## ğŸ¤ Contributing

### Development Setup

```bash
# Clone repository
git clone https://github.com/yourusername/StockExperiment.git
cd StockExperiment

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Start development server
./run.sh dev
```

### Code Style

- **Python:** PEP 8, type hints preferred
- **SQL:** Uppercase keywords, snake_case tables
- **JavaScript:** ES6, camelCase
- **Comments:** Docstrings for functions, inline comments for complex logic

### Testing

```bash
# Run tests (when implemented)
pytest tests/

# Code coverage
pytest --cov=src tests/

# Linting
flake8 src/
pylint src/
```

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file for details.

---

## ğŸ‰ Summary

âœ… **100% Automated** - Data collection, ML training, stock selection
âœ… **2,259 Stocks** - Complete fundamental and technical data
âœ… **1-Year Historical** - 820K+ OHLCV records with indicators
âœ… **ML Predictions** - Price targets, risk scores, confidence levels
âœ… **Daily Top 50** - Automated stock picks with ML
âœ… **CSV Exports** - Daily backups for analysis
âœ… **Admin Dashboard** - Manual controls and monitoring
âœ… **REST API** - Fast, cached endpoints
âœ… **Self-Healing** - Auto-restart, retry failed steps
âœ… **Production-Ready** - Docker, logging, error handling

**Zero manual intervention required!** ğŸš€

---

## ğŸ“ Support

### Getting Help

1. Check documentation in `/docs` folder
2. Review logs: `docker compose logs -f`
3. Check system status: `./tools/check_all_schedulers.sh`
4. Search issues on GitHub
5. Create new issue with logs and details

### Common Commands

```bash
# Start system
./run.sh dev

# Stop system
docker compose down

# Restart services
docker compose restart data_scheduler ml_scheduler trading_system

# View logs
docker compose logs -f [service_name]

# Check status
./tools/check_all_schedulers.sh

# Manual tasks
python3 run_pipeline.py
python3 tools/train_ml_model.py

# Database access
docker exec -it trading_system_db_dev psql -U trader -d trading_system
```

---

**Built with â¤ï¸ for automated stock trading and analysis**

Last Updated: October 4, 2025
