# Project Structure

## üìÅ Directory Layout

```
/StockExperiment
‚îú‚îÄ‚îÄ README.md                    # Quick start guide
‚îú‚îÄ‚îÄ app.py                       # Flask application entry point
‚îú‚îÄ‚îÄ run.py                       # Application launcher
‚îú‚îÄ‚îÄ run.sh                       # Docker startup script
‚îú‚îÄ‚îÄ config.py                    # Application configuration
‚îú‚îÄ‚îÄ run_pipeline.py              # Data pipeline orchestrator (6-step saga)
‚îú‚îÄ‚îÄ data_scheduler.py            # Data automation (9 PM daily)
‚îú‚îÄ‚îÄ scheduler.py                 # ML automation (2 AM daily)
‚îÇ
‚îú‚îÄ‚îÄ /src                         # Source code
‚îÇ   ‚îú‚îÄ‚îÄ /models                  # Database models (SQLAlchemy)
‚îÇ   ‚îú‚îÄ‚îÄ /services               # Business logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ /data               # Data pipeline, sagas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ /broker             # Fyers API integration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ /ml                 # Machine learning models
‚îÇ   ‚îú‚îÄ‚îÄ /web                    # Flask routes & templates
‚îÇ   ‚îî‚îÄ‚îÄ /utils                  # Helpers, logging
‚îÇ
‚îú‚îÄ‚îÄ /config                      # YAML configurations
‚îÇ   ‚îî‚îÄ‚îÄ stock_filters.yaml      # Screening criteria
‚îÇ
‚îú‚îÄ‚îÄ /docker                      # Docker files
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml      # Container orchestration
‚îÇ
‚îú‚îÄ‚îÄ /tools                       # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ train_ml_model.py       # Manual ML training
‚îÇ   ‚îú‚îÄ‚îÄ check_scheduler.sh      # ML scheduler status
‚îÇ   ‚îî‚îÄ‚îÄ check_all_schedulers.sh # Complete system status
‚îÇ
‚îú‚îÄ‚îÄ /docs                        # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ AUTOMATION.md           # Complete automation guide
‚îÇ   ‚îú‚îÄ‚îÄ ML_GUIDE.md             # Machine learning guide
‚îÇ   ‚îî‚îÄ‚îÄ STRUCTURE.md            # This file
‚îÇ
‚îú‚îÄ‚îÄ /logs                        # Application logs
‚îÇ   ‚îú‚îÄ‚îÄ data_scheduler.log      # Data pipeline logs
‚îÇ   ‚îî‚îÄ‚îÄ scheduler.log           # ML scheduler logs
‚îÇ
‚îî‚îÄ‚îÄ /exports                     # CSV exports (daily)
    ‚îú‚îÄ‚îÄ stocks_YYYY-MM-DD.csv
    ‚îú‚îÄ‚îÄ historical_30d_YYYY-MM-DD.csv
    ‚îî‚îÄ‚îÄ suggested_stocks_YYYY-MM-DD.csv
```

---

## üóÑÔ∏è Database Tables

### Core Tables (Created by SQLAlchemy)

#### `stocks`
Primary stock data with fundamentals
- symbol, stock_name, current_price, market_cap
- pe_ratio, pb_ratio, roe, eps, beta
- revenue_growth, earnings_growth, margins
- Last updated: Real-time via pipeline

#### `historical_data`
1-year OHLCV data for all stocks
- symbol, date, open, high, low, close, volume
- Records: ~2,259 stocks √ó 365 days = ~820K rows

#### `technical_indicators`
Calculated indicators for each stock/day
- rsi_14, macd, signal_line, macd_histogram
- sma_50, sma_200, ema_12, ema_26, atr_14
- Records: Same as historical_data

#### `daily_suggested_stocks`
ML predictions & daily top picks
- date, symbol, strategy, rank
- ml_prediction_score, ml_price_target
- ml_confidence, ml_risk_score
- Records: 50 stocks/day √ó 90 days = ~4,500 rows

#### `pipeline_execution_tracking`
Pipeline saga execution logs
- pipeline_id, step_name, status
- start_time, end_time, error_message

### User & Trading Tables
- `users` - User accounts
- `strategies` - Trading strategies
- `orders` - Order history
- `trades` - Executed trades
- `positions` - Current positions
- `broker_configurations` - API credentials

---

## üîÑ Data Flow

### Daily Pipeline (9 PM - 10:30 PM)

```
data_scheduler.py
    ‚Üì
run_pipeline.py (6-step saga)
    ‚Üì
Step 1: SYMBOL_MASTER
    ‚Üí Fetch NSE symbols from Fyers
    ‚Üí Store in stocks table
    ‚Üì
Step 2: STOCKS
    ‚Üí Fetch current prices, market cap
    ‚Üí Update stocks table
    ‚Üì
Step 3: HISTORICAL_DATA
    ‚Üí Fetch 1-year OHLCV for all stocks
    ‚Üí Store in historical_data table
    ‚Üì
Step 4: TECHNICAL_INDICATORS
    ‚Üí Calculate RSI, MACD, SMA, EMA, ATR
    ‚Üí Store in technical_indicators table
    ‚Üì
Step 5: COMPREHENSIVE_METRICS
    ‚Üí Calculate volatility metrics
    ‚Üí Update stocks table
    ‚Üì
Step 6: PIPELINE_VALIDATION
    ‚Üí Verify data quality
    ‚Üí Log to pipeline_execution_tracking
    ‚Üì
fill_data_sql.py
    ‚Üí Populate missing fields
    ‚Üì
fix_business_logic.py
    ‚Üí Calculate derived metrics
    ‚Üì
export_daily_csv()
    ‚Üí Export to /exports
```

### ML Pipeline (2 AM - 3 AM)

```
scheduler.py
    ‚Üì
Step 1: train_ml_models()
    ‚Üí Fetch 365 days historical + technical
    ‚Üí Train Random Forest models
    ‚Üí Store in memory
    ‚Üì
Step 2: update_daily_snapshot()
    ‚Üí Run suggested_stocks_saga
        ‚Üì
        Step 1-5: Filter & score stocks
        Step 6: Apply ML predictions
        Step 7: Save top 50 to daily_suggested_stocks
    ‚Üì
Step 3: cleanup_old_snapshots() (Sunday only)
    ‚Üí Remove snapshots > 90 days old
```

---

## üöÄ Key Scripts

### Production Scripts (Root)

**`run.sh`**
- Starts Docker containers
- Launches Flask app
- Usage: `./run.sh dev` or `./run.sh prod`

**`run_pipeline.py`**
- Orchestrates 6-step data pipeline
- Called by data_scheduler.py
- Duration: 20-30 minutes

**`data_scheduler.py`**
- Automated data collection (9 PM daily)
- Runs: pipeline ‚Üí fill ‚Üí calc ‚Üí export
- Docker service: `data_scheduler`

**`scheduler.py`**
- Automated ML training (2 AM daily)
- Runs: train ‚Üí snapshot ‚Üí cleanup
- Docker service: `ml_scheduler`

### Utility Scripts (/tools)

**`train_ml_model.py`**
- Manual ML model training
- Usage: `python3 tools/train_ml_model.py`
- Duration: 1-2 minutes

**`check_all_schedulers.sh`**
- Complete system status check
- Shows: containers, database, exports, logs
- Usage: `./tools/check_all_schedulers.sh`

**`check_scheduler.sh`**
- ML scheduler status only
- Usage: `./tools/check_scheduler.sh`

---

## üìä Configuration Files

### `/config/stock_filters.yaml`
Stock screening criteria:
```yaml
min_price: 50.0
max_price: 5000.0
min_market_cap_crores: 500
min_volume: 100000
min_rsi: 30
max_rsi: 70
```

### `.env`
Environment variables:
```bash
FYERS_CLIENT_ID=your_client_id
FYERS_ACCESS_TOKEN=your_access_token
DATABASE_URL=postgresql://...
```

### `docker-compose.yml`
Services configuration:
- database (PostgreSQL)
- dragonfly (Redis)
- trading_system (Flask API)
- data_scheduler (Automation)
- ml_scheduler (ML Automation)

---

## üåê API Endpoints

### Main Endpoints

**`GET /api/suggested-stocks/`**
- Returns top stocks with ML predictions
- Query params: `strategy`, `limit`, `sector`, `sort_by`
- Response: Stocks with ML scores

**`GET /admin/`**
- Admin dashboard (admin users only)
- Manual task triggers
- Real-time status monitoring

**`GET /admin/system/status`**
- System statistics (JSON)
- Stock counts, latest updates

**`POST /admin/trigger/{task_type}`**
- Manual task execution
- Types: pipeline, fill-data, business-logic, ml-training, csv-export, all

---

## üìù Logs

### Log Files (`/logs`)

**`data_scheduler.log`**
- Data pipeline execution logs
- CSV exports
- Data quality checks

**`scheduler.log`**
- ML training logs
- Daily snapshot creation
- Cleanup operations

**`app.log`** (if configured)
- Flask application logs
- API requests
- Errors

### Viewing Logs

```bash
# Real-time (all services)
docker compose logs -f

# Specific service
docker compose logs -f data_scheduler
docker compose logs -f ml_scheduler

# Last 100 lines
docker compose logs --tail=100 data_scheduler
```

---

## üéØ Quick Reference

### Start System
```bash
./run.sh dev
```

### Check Status
```bash
./tools/check_all_schedulers.sh
```

### Manual Tasks
```bash
# Data pipeline
python3 run_pipeline.py

# ML training
python3 tools/train_ml_model.py

# Via Admin Dashboard
http://localhost:5001/admin
```

### Database Access
```bash
# PostgreSQL
docker exec -it trading_system_db_dev psql -U trader -d trading_system

# Redis
docker exec -it trading_system_redis redis-cli
```

---

## üìö Documentation

- **`README.md`** (root) - Quick start guide
- **`docs/AUTOMATION.md`** - Complete automation guide
- **`docs/ML_GUIDE.md`** - Machine learning guide
- **`docs/STRUCTURE.md`** - This file
- **`tools/README.md`** - Tools documentation

---

## üê≥ Docker Services

```yaml
services:
  database:
    image: postgres:15
    port: 5432
    data: /var/lib/postgresql/data

  dragonfly:
    image: docker.dragonflydb.io/dragonflydb/dragonfly
    port: 6379
    purpose: Redis cache

  trading_system:
    build: .
    port: 5001
    purpose: Flask API

  data_scheduler:
    command: python data_scheduler.py
    purpose: Data automation

  ml_scheduler:
    command: python scheduler.py
    purpose: ML automation
```

---

## üéØ Development Workflow

1. **Start containers:** `./run.sh dev`
2. **Access API:** `http://localhost:5001`
3. **Access Admin:** `http://localhost:5001/admin`
4. **Check logs:** `docker compose logs -f`
5. **Run pipeline manually:** `python3 run_pipeline.py`
6. **Train ML manually:** `python3 tools/train_ml_model.py`

---

## ‚úÖ File Count Summary

- **Root Python scripts:** 6 (production)
- **Tools scripts:** 3 (utilities)
- **Documentation:** 4 files
- **Source code:** ~50+ Python files
- **Config files:** 3-4 YAML files
- **Docker files:** 1 docker-compose.yml

**Total:** Clean, organized, production-ready structure! üéâ
